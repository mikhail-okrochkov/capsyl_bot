{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b47f4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from openai import OpenAI   \n",
    "import ast        # assumes correct openai CLIP is installed\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import dateparser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e3a24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/google_photos_metadata_with_location.xlsx\")\n",
    "metadata_embeddings = np.load(\"../data/metadata_embeddings.npy\")\n",
    "image_embeddings = np.load(\"../data/image_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07c7ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"city\"] = df[\"city\"].astype(str)\n",
    "df[\"country\"] = df[\"country\"].astype(str)\n",
    "\n",
    "# Combine into location_name, skip NaN or empty strings\n",
    "def make_location_name(row):\n",
    "    parts = []\n",
    "    for col in [\"city\", \"country\"]:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val.strip() != \"nan\" and val.strip() != \"\":\n",
    "            parts.append(val.strip())\n",
    "    return \", \".join(parts)\n",
    "\n",
    "df[\"location_name\"] = df.apply(make_location_name, axis=1)\n",
    "\n",
    "# Optional: lowercase for easier matching\n",
    "df[\"location_name\"] = df[\"location_name\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c804a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c2cb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d29710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_text(text_tokens)\n",
    "        emb /= emb.norm(dim=-1, keepdim=True)\n",
    "    return emb.cpu().numpy()[0]\n",
    "\n",
    "def cosine_similarity(query_emb, embeddings):\n",
    "    return embeddings.dot(query_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4643e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_photos_gpt(user_message, top_k=5, df_subset=None):\n",
    "    # 1. Extract structured filters\n",
    "    filters = parse_user_query(user_message)\n",
    "    print(filters)\n",
    "    year = filters.get(\"year\")\n",
    "    month = filters.get(\"month\")\n",
    "    people = filters.get(\"people\", [])\n",
    "    location = filters.get(\"location\")\n",
    "    keywords = filters.get(\"keywords\", [])\n",
    "\n",
    "    # 2. Determine dataset to search\n",
    "    df_search = df_subset if df_subset is not None else df.copy()\n",
    "\n",
    "    # 3. Apply filters\n",
    "    if year:\n",
    "        df_search = df_search[df_search[\"datetime\"].dt.year == year]\n",
    "    if month:\n",
    "        df_search = df_search[df_search[\"datetime\"].dt.month == month]\n",
    "    if people:\n",
    "        df_search = df_search[df_search[\"names_list\"].apply(lambda x: any(p in x for p in people) if x else False)]\n",
    "    if location:\n",
    "        loc_lower = location.lower()\n",
    "        df_search = df_search[df_search[\"location_name\"].str.contains(loc_lower, na=False)]\n",
    "\n",
    "    if len(df_search) == 0:\n",
    "        return []\n",
    "\n",
    "    # 4. Embeddings for filtered set\n",
    "    indices = df_search.index.to_numpy()\n",
    "    img_emb_search = image_embeddings[indices]\n",
    "    meta_emb_search = metadata_embeddings[indices]\n",
    "\n",
    "    # 5. Embed user query\n",
    "    q_emb = embed_text(user_message)\n",
    "\n",
    "    # 6. Compute initial similarity (image + metadata)\n",
    "    sim_meta = cosine_similarity(q_emb, meta_emb_search)\n",
    "    sim_img = cosine_similarity(q_emb, img_emb_search)\n",
    "    sim_combined = (sim_meta + sim_img) / 2.0\n",
    "\n",
    "    # 7. Incorporate keyword embeddings if present\n",
    "    if keywords:\n",
    "        keyword_embs = np.array([embed_text(k) for k in keywords])\n",
    "        # Average similarity across all keywords\n",
    "        sim_keywords = np.mean([cosine_similarity(k_emb, meta_emb_search) for k_emb in keyword_embs], axis=0)\n",
    "        # Combine with existing similarity (weight can be adjusted)\n",
    "        sim_combined = (sim_combined + sim_keywords) / 2.0\n",
    "\n",
    "    # 8. Top-k results\n",
    "    idx_sorted = np.argsort(-sim_combined)[:top_k]\n",
    "    results = []\n",
    "    for rank, idx in enumerate(idx_sorted):\n",
    "        row = df_search.iloc[idx].to_dict()\n",
    "        row[\"_score\"] = float(sim_combined[idx])\n",
    "        row[\"_rank\"] = int(rank + 1)\n",
    "        results.append(row)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def parse_user_query(user_message):\n",
    "    system_prompt = \"\"\"\n",
    "    You are an assistant that extracts structured search filters from a user's natural language query.\n",
    "    Always respond with a JSON object with the following keys:\n",
    "    - year (int or null)\n",
    "    - month (int 1-12 or null)\n",
    "    - people (list of strings)\n",
    "    - location (string or null)\n",
    "    - keywords (list of strings)\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    text = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        filters = json.loads(text)\n",
    "    except Exception:\n",
    "        filters = {}\n",
    "    \n",
    "    return filters\n",
    "\n",
    "\n",
    "history = {\n",
    "    \"messages\": [],\n",
    "    \"last_results\": None  # store previous top-k results\n",
    "}\n",
    "\n",
    "def chat_with_photos(user_message, top_k=5, use_previous_results=True):\n",
    "    # Decide which dataset to search\n",
    "    df_subset = None\n",
    "    if use_previous_results and history[\"last_results\"]:\n",
    "        df_subset = pd.DataFrame(history[\"last_results\"])\n",
    "\n",
    "    # Search photos\n",
    "    results = search_photos_gpt(user_message, top_k=top_k, df_subset=df_subset)\n",
    "\n",
    "    # Build system prompt for GPT-5\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful photo assistant.\n",
    "    Here are the top {top_k} candidate photos from the user's library:\n",
    "    {results}\n",
    "    Respond naturally to the user's question and reference relevant photo names.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = history[\"messages\"] + [{\"role\": \"user\", \"content\": user_message}]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    bot_message = response.choices[0].message.content\n",
    "\n",
    "    # Update conversation history\n",
    "    history[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n",
    "    history[\"messages\"].append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "    history[\"last_results\"] = results  # save for next query\n",
    "\n",
    "    return bot_message, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75417c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': 2025, 'month': None, 'people': ['Alesia'], 'location': 'Hawaii', 'keywords': ['beach']}\n"
     ]
    }
   ],
   "source": [
    "msg = \"Show me photos of Alesia in Hawaii from 2025 on the beach\"\n",
    "reply, results = chat_with_photos(msg, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
